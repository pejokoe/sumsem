{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978a01c",
   "metadata": {
    "id": "3978a01c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) # Set the random seed so results are always the same when re-running the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c101e6",
   "metadata": {
    "id": "68c101e6"
   },
   "source": [
    "# Practical Machine Learning introduction with Python\n",
    "\n",
    "Machine learning (ML) is the study of algorithms allowing computers to perform a task without being explicitly programmed for it but instead by learning from data.\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "The most common field of ML algorithms are the supervised learning algorithms. Supervised algorithms learn to perform a task by using a dataset of input/output pairs.\n",
    "\n",
    "In practice, if we have a dataset of m \"training examples\" (a pair of input/output), we can represent it as\n",
    "\n",
    "$(\\vec{x^i},y^i)$,\n",
    "\n",
    "where $i$ is the index of the pair of input/output and ranges over $1, ..., m$, where $m$ is the amount of $(\\vec{x},y)$ pairs present in the dataset. $\\vec{x}$ is an input and $y$ is an output.\n",
    "\n",
    "The goal of a supervised algorithm is to find a good hypothesis function $h$ representing the relation $h(\\vec{x})=y$. In practice, we cannot hope to find a perfect hypothesis function $h$ such that $h(\\vec{x^i})=y^i$ for all i in our dataset and that would also be true for new data samples drawn from a different dataset. Instead, the hypothesis function $h$ only gives an approximation $\\hat{y}$ of the true value $y$, so we have $h(\\vec{x})=\\hat{y}$. Supervised ML algorithms allows us to find a good hypothesis function $h$ that reduce as much as possible the error between the predictions $\\hat{y}^i$ and the true values $y^i$.\n",
    "\n",
    "Another way to represent the dataset $(\\vec{x^i},y^i)^{i=1,...,m}$ is to use the matrix form: $(X,\\vec{y})$, where $X=(x^i_j)^{i=1,...,m}_{j=1,...,n}$ and $\\vec{y}=(y^i)^{i=1,...,m}$. So $X$ is a $m$x$n$ matrix of $m$ vectors $x^i$ of dimension $n$, and $\\vec{y}$ is a vector of dimension $m$.\n",
    "\n",
    "## Regression\n",
    "\n",
    "There are two main types of tasks that supervised algorithms can perform:  regression or classification. The choice depends on the data type of the output $y$. If $y$ is continuous ($y \\in \\mathbb{R}$), then we are performing a regression. \n",
    "\n",
    "As an example application of regression, we will try to estimate the median housing price of an area from different housing attributes of this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b830fb5",
   "metadata": {
    "id": "6b830fb5"
   },
   "source": [
    "Scikit-learn is a Python library offering a great variety of ML algorithms. Its \"datasets\" module also offer a quick way to load a few \"toy\" datasets  (i.e. simple datasets useful for education and for testing ML algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b5baf",
   "metadata": {
    "id": "d62b5baf"
   },
   "outputs": [],
   "source": [
    "####################### Tip ###############################################\n",
    "# We load the California Housing dataset using the scikit-learn library\n",
    "###########################################################################\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing_dataset = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d7148",
   "metadata": {
    "id": "546d7148"
   },
   "source": [
    "Let's have a look at this dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea6f68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37ea6f68",
    "outputId": "b0e1ad4b-c0b0-4f3d-9799-99a461b19c01",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'frame': None,\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nAn household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surpinsingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "california_housing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be5b61",
   "metadata": {
    "id": "c2be5b61"
   },
   "source": [
    "The dataset has a dictionary form.\n",
    "\n",
    "Using the different keys, we can recover our inputs $X$ ('data'), our targets $\\vec{y}$ ('target'), the name of each attributes for $X$ ('feature_names') and a descriptive string about the dataset ('DESCR')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a44857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6a44857",
    "outputId": "670a403b-c455-4fdd-fb55-d1644cd99baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "An household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surpinsingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################### Tip ##########################\n",
    "# column = california_housing_dataset ['column_name']\n",
    "######################################################\n",
    "X = california_housing_dataset[\"data\"]\n",
    "y = california_housing_dataset[\"target\"]\n",
    "feature_names = california_housing_dataset['feature_names']\n",
    "\n",
    "print(california_housing_dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74448225",
   "metadata": {
    "id": "74448225"
   },
   "source": [
    "From the description of this dataset, we learn that there are $m=20640$ pairs of $(\\vec{x},y)$. Each $\\vec{x}$ is an 8-dimensional vector representing different housing attributes, and $y$ is the median house value for a district in \\\\$100,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28305d05",
   "metadata": {
    "id": "28305d05"
   },
   "source": [
    "## A little deviation about NumPy\n",
    "\n",
    "Python is a very popular programming language in science and particularly in ML but it is not due to its computing speed. In fact, Python is quite slow if we want to use it in itself to compute mathematical operations such as matrix or vector multiplication and addition, which are the most frequent type of operation in ML algorithms. What makes Python popular is its large choice of libraries giving access to powerful algorithms written in some other more computationally efficient languages, such as C or Fortran for example.\n",
    "\n",
    "Numpy is the most important Python library for mathematical operations. It allows to transform Python lists into a numpy object, more specifically a numpy ndarray (an $n$-dimensional array), for which Numpy allows us to apply a variety of mathematical operations very easily and very efficiently in terms of computing speed (much more than if we were to write everything in Python code). Numpy is a central Python library and most of the other scientific Python libraries also use its ndarray data format.\n",
    "\n",
    "I invite you to check the introduction to the Numpy library: https://numpy.org/doc/stable/user/absolute_beginners.html\n",
    "\n",
    "Let's see a few operations that can be done using Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981581ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "981581ba",
    "outputId": "d216b6b8-b421-4da0-c9fb-7858be665f05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################### Tip ##########################\n",
    "# we can create a vector from a python list like this\n",
    "######################################################\n",
    "vector_a = [1,2,3]\n",
    "vector_a = np.array(vector_a)\n",
    "\n",
    "vector_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbbf033",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bbbf033",
    "outputId": "cdb4e19d-b06c-462f-d0de-841b29d9a704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] + [1 1 1] = [2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# or directly like this\n",
    "vector_b = np.array([4,5,6])\n",
    "\n",
    "# there is a bunch of other ways to create arrays\n",
    "vector_c = np.ones(3, dtype=int) # the argument is the dimensional shape of the tensor, here we create a vector of dim=3\n",
    "\n",
    "# then we can do operations such as additions\n",
    "print(f\"{vector_a} + {vector_c} = {vector_a + vector_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3771c8",
   "metadata": {
    "id": "7e3771c8"
   },
   "source": [
    "We can do the Hadamard product (or element wise multiplication): $ \\vec{a} \\odot \\vec{b} = (a^i * b^i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20265a52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20265a52",
    "outputId": "c8bc0a5a-ddd0-4475-df19-7edfc9e477fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] * [4 5 6] = [ 4 10 18]\n"
     ]
    }
   ],
   "source": [
    "# Hadamard product\n",
    "\n",
    "print(f\"{vector_a} * {vector_b} = {vector_a * vector_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbff5cc",
   "metadata": {
    "id": "0dbff5cc"
   },
   "source": [
    "Or the dot product: $\\vec{a} \\cdot \\vec{b} = \\sum_{i} a_i b_i$\n",
    "<img src=\"https://d138zd1ktt9iqe.cloudfront.net/media/seo_landing_files/matrix-representation-of-dot-product-1626103121.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddb6f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaddb6f7",
    "outputId": "b309980f-6895-4b9d-e2ed-324915a4c9e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dot product \n",
    "np.dot(vector_a,vector_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b6c37",
   "metadata": {
    "id": "052b6c37"
   },
   "source": [
    "Let's see a few examples of operations using matrices now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be49c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31be49c1",
    "outputId": "103a75b8-73a0-47bd-81b6-ebe441ab2517"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################### Tip ##########################\n",
    "# np.ones(Rows, Columns)\n",
    "######################################################\n",
    "A = np.ones((4,3),dtype=int) \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b808d52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b808d52",
    "outputId": "f5941df1-ecd9-4b3d-b3e7-833604e6b0f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check its shape (= its dimensions)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e1873",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b19e1873",
    "outputId": "5756e659-447c-4412-a52c-e0a1eb8451cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarly for our vector a\n",
    "vector_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2784830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2784830",
    "outputId": "cd313a71-2348-485e-df6b-adf2018a771b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can perform the matrix multiplication\n",
    "A.dot(vector_a) # it is the same than doing np.dot(A, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc0d39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93bc0d39",
    "outputId": "62de9114-b827-4a28-f64a-e568b7ab222f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854],\n",
       "       [ 1.52302986, -0.23415337, -0.23413696],\n",
       "       [ 1.57921282,  0.76743473, -0.46947439],\n",
       "       [ 0.54256004, -0.46341769, -0.46572975]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can generate a random matrix with values drawn from a normal distribution [0,1)\n",
    "B = np.random.randn(4,3)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a8c77",
   "metadata": {
    "id": "1c4a8c77"
   },
   "source": [
    "Numpy allows us to obtain easily the transposed of a matrix, to perform for example the product $A^T \\cdot B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40c2db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea40c2db",
    "outputId": "6a784298-6596-4d0a-e021-d9b169fbfb0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.14151687, -0.06840064, -0.52165256],\n",
       "       [ 4.14151687, -0.06840064, -0.52165256],\n",
       "       [ 4.14151687, -0.06840064, -0.52165256]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A.T, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49a8a5",
   "metadata": {
    "id": "6c49a8a5"
   },
   "source": [
    "Now, let's get back to our housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3717301",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3717301",
    "outputId": "cd12ae01-7bb1-4983-f185-dd508de843cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check what type of object is X\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fec9f5",
   "metadata": {
    "id": "46fec9f5"
   },
   "source": [
    "As you can see, our dataset loaded from the scikit learn library is already in a numpy ndarray format. Let's explore a bit our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e386dc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e386dc9",
    "outputId": "cfdccd8c-b794-42d0-d8ec-a4589056a223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (20640, 8)\n",
      "y shape: (20640,)\n",
      "feature_names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "x0 = [   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n",
      "y0 = 4.526\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"feature_names:\", feature_names)\n",
    "print(\"x0 =\", X[0])\n",
    "print(\"y0 =\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ebaab",
   "metadata": {
    "id": "661ebaab"
   },
   "source": [
    "## Introducing Pandas\n",
    "\n",
    "A useful library when working with tabular data is Pandas. It is basically Excel but in Python. Its advantage is that we can attribute a name to each column in our dataset, so we can use the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2755a92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "c2755a92",
    "outputId": "23786077-3ba0-4f59-805d-6524d7628a31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c3d367e9-a4d2-4eff-a736-c81d1c60fa65\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3d367e9-a4d2-4eff-a736-c81d1c60fa65')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c3d367e9-a4d2-4eff-a736-c81d1c60fa65 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c3d367e9-a4d2-4eff-a736-c81d1c60fa65');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# A pandas dataframe object is a table, to which we can assign names to the columns\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f50ab7",
   "metadata": {
    "id": "29f50ab7"
   },
   "source": [
    "Pandas allows to perform different transformations to a table (called a \"dataframe\" in pandas terminology), and the main interest of this library is that we can more easily identify each column by its name, instead of its index. If we don't have or just don't care about the name of each columns, pandas is not really useful in this context.\n",
    "\n",
    "Usually, real life datasets are messy. There is some missing data, sometimes some wrong values and very often there is some useless data. Also, sometimes the data is not numeric: it can be the string data type, which we cannot simply feed as input to our ML model. For all these reasons, it is almost always necessary to transform the dataset before it is usable by our model, and pandas can be a useful library for that.\n",
    "\n",
    "To learn more about pandas: https://pandas.pydata.org/docs/user_guide/10min.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6fddf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ab6fddf",
    "outputId": "e5d14e6e-5f40-4173-dc35-1c96fb6bfe20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        8.3252\n",
       "1        8.3014\n",
       "2        7.2574\n",
       "3        5.6431\n",
       "4        3.8462\n",
       "          ...  \n",
       "20635    1.5603\n",
       "20636    2.5568\n",
       "20637    1.7000\n",
       "20638    1.8672\n",
       "20639    2.3886\n",
       "Name: MedInc, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas allows us to select a column by its name\n",
    "X_df[\"MedInc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1338dcb",
   "metadata": {
    "id": "e1338dcb"
   },
   "source": [
    "A single column is a 1-dimensional array, or simply a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a17dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "d36a17dd",
    "outputId": "3869288c-030b-41bf-ed04-60edaa4c2f5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8550035c-3093-4992-98fd-c93e024979de\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8550035c-3093-4992-98fd-c93e024979de')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8550035c-3093-4992-98fd-c93e024979de button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8550035c-3093-4992-98fd-c93e024979de');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       MedInc  Population  AveOccup\n",
       "0      8.3252       322.0  2.555556\n",
       "1      8.3014      2401.0  2.109842\n",
       "2      7.2574       496.0  2.802260\n",
       "3      5.6431       558.0  2.547945\n",
       "4      3.8462       565.0  2.181467\n",
       "...       ...         ...       ...\n",
       "20635  1.5603       845.0  2.560606\n",
       "20636  2.5568       356.0  3.122807\n",
       "20637  1.7000      1007.0  2.325635\n",
       "20638  1.8672       741.0  2.123209\n",
       "20639  2.3886      1387.0  2.616981\n",
       "\n",
       "[20640 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also select a list of columns, returning a datafram\n",
    "X_df[[\"MedInc\", \"Population\", \"AveOccup\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb83e9",
   "metadata": {
    "id": "f0eb83e9"
   },
   "source": [
    "## Divising our dataset into a training, validation and test set\n",
    "\n",
    "Before training our first model, we need to split the dataset into a training, a validation and a test set. We need to do this to avoid overfitting our dataset, i.e. finding a model that performs well only on our training dataset and not well on new data unseen during training by our model. Overfitting is a problem that you are more likely to experience if you use a ML model that is too complex (with many parameters) compared to the amount of training data samples (too few). Your model can also suffer from the opposite problem, which is underfitting. This happens if you choose a model that is too simple to represent the (more complex) relationships present in the training data available (a lot of data). By splitting our dataset into 3 parts, we can avoid both overfitting and underfitting and select a good model that will perform well on new unseen data. \n",
    "\n",
    "The training set is obviously the subset of data used to train the model (to optimise its parameters), the validation set is used to evaluate and intercompare different candidate models and then select the model performing the best on this dataset. Then, if we have evaluated a lot of models on our validation subset of data, it may be possible that this model was simply \"lucky\" on this validation dataset, so to have a final independent evaluation of that model, we finally use the test set for evaluation.\n",
    "\n",
    "The ratio of data samples allocated to each of the training/validation/test set depends on the problem and the quantity of data available in total. In general, we want as much as possible data for training, while keeping enough data in the validation and test set so that the evaluation of the models stays statistically meaningful.\n",
    "\n",
    "In the case of our housing dataset, we have about 20000 data samples, a good data spliting ratio is 80% / 10% / 10%.\n",
    "\n",
    "#### Important remark\n",
    "\n",
    "Scikit-learn has function that allows to split easily our dataset into two parts, using it twice will allow us to have our three datasets. Also, this function **splits randomly**. This is very important to do if you want to sample subsets that are statistically independent. For example, if the dataset was ordered in term of house pricing, and you would take the last 10% of data samples in your test set, your test set would be the 10% most expensive house districts, which would not actually represent well the statistics of all the house districts in California.\n",
    "\n",
    "Using this function will also give you a randomly sorted training dataset, which is necessary for training some specific ML models, such as those using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3fd01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aa3fd01",
    "outputId": "78f87bc3-d1c7-4e5b-a829-bcc62cbc86af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data samples for training: 16512\n",
      "# of data samples for validation: 2064\n",
      "# of data samples for testing: 2064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5)\n",
    "\n",
    "print(\"# of data samples for training:\", len(X_train))\n",
    "print(\"# of data samples for validation:\", len(X_val))\n",
    "print(\"# of data samples for testing:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109889e",
   "metadata": {
    "id": "a109889e"
   },
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "Before training our first model, it is always helpful to explore the data a bit further than just reading its descriptive file. The minimum is to learn a bit more about the main statistics of our dataset, and particularly for the target data (the output data, or y if you prefer). Because learning about the statistics of the dataset can guide your choice toward some particular ML algorithm, you should only explore the training dataset.\n",
    "\n",
    "We will begin by computing the statistics about the y_train values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fe4ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "179fe4ab",
    "outputId": "40013f25-f6d9-4151-dfe7-7947110f0544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about y training data\n",
      "mean: 2.0729028439922477\n",
      "standard deviation: 1.1558734503987778\n",
      "minimum: 0.14999\n",
      "maximum: 5.00001\n"
     ]
    }
   ],
   "source": [
    "print(\"Statistics about y training data\")\n",
    "print(\"mean:\", np.mean(y_train))\n",
    "print(\"standard deviation:\", np.std(y_train))\n",
    "print(\"minimum:\", np.min(y_train))\n",
    "print(\"maximum:\", np.max(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb7076",
   "metadata": {
    "id": "1afb7076"
   },
   "source": [
    "So, there is some district where the median housing price is only \\$15 000... That is quite cheap. Also, the maximum median housing price for a district is only \\$500 000, that is not that much (we are talking about California here, so that includes Hollywood, LA, San Francisco, etc...) but let's remember that this dataset is quite old, being from 1990. The average housing price is almost in the middle of the two extremum values, so maybe our data is normally distributed ? To verify this we will need to do a plot.\n",
    "\n",
    "### Visualizing data\n",
    "\n",
    "For plotting your data, you won't escape the library matplotlib (https://matplotlib.org/3.5.0/tutorials/introductory/pyplot.html). Let's plot a simple histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f96c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "978f96c9",
    "outputId": "5c24926a-6646-4bcb-b7c4-4f03c56ecf8a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(y_train, 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218d9e2",
   "metadata": {
    "id": "1218d9e2"
   },
   "source": [
    "We see that the data is not really normally distributed, it is skewed to the right here. We can also see that there is an anormally high frequency of prices at about \\$500 000. The maximum value for the prices is also suspect because it exactly \\$500 000. This indicates that there is a maximum threshold that was applied to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb0875",
   "metadata": {
    "id": "04bb0875"
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "The first example of supervised algorithm that we are going to see is the linear regression.\n",
    "\n",
    "Scikit-learn is a library offering a large collection of ML algorithms and tools to perform a typical ML application. A quick tutorial of Scikit-learn is available at https://scikit-learn.org/stable/getting_started.html and a list of the supervised algorithm offered by Scikit-learn can be found here https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "\n",
    "We will begin with the most simple form of ML model for a regression, which is the linear regression: $\\hat{y} = \\vec{w} \\cdot \\vec{x} + b$, where $\\vec{w}$ and $b$ are the parameters of the model. ML algorithms optimise their parameters with a dataset by minimising a **loss function**, i.e. a measure of the distance between the estimations made by the model $\\vec{\\hat{y}}$ and the true values $\\vec{y}$. In the case of a linear regression, we select the parameters $\\vec{w}$ and $b$ that minimise the mean squared error (mse) between $\\vec{\\hat{y}}$ and $\\vec{y}$.\n",
    "\n",
    "After importing the linear regression model from scikit-learn, we need to create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f3cec",
   "metadata": {
    "id": "f58f3cec"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503e490",
   "metadata": {
    "id": "5503e490"
   },
   "source": [
    "### Training a model\n",
    "\n",
    "Then, to train the model, we only need to call the method \"fit\" and pass the training dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc5a25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cdc5a25",
    "outputId": "ec36616b-951a-4d47-8eda-d877f189db3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50961d4",
   "metadata": {
    "id": "e50961d4"
   },
   "source": [
    "It is as simple as that, and this is the same for all ML models (called \"estimators\" in the scikit-learn terminology) from the scikit-learn library. Just keep in mind that this simple function call hides a complex optimization or search procedure such as the one we have seen in Module 4, in order to find the right parameters of the ML algorithm.\n",
    "\n",
    "We can obtain the parameters found after training like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f2d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b7f2d36",
    "outputId": "72d21eab-aeb3-462f-e880-41821ca99896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 4.44469130e-01  9.49985492e-03 -1.18314950e-01  7.80177574e-01\n",
      " -2.73454672e-06 -3.42848457e-03 -4.26598312e-01 -4.39284987e-01]\n",
      "b= -37.44733469895107\n"
     ]
    }
   ],
   "source": [
    "w = lin_reg.coef_\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "print(\"w = \", w)\n",
    "print(\"b=\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada1616",
   "metadata": {
    "id": "9ada1616"
   },
   "source": [
    "### Making new predictions with a trained model and evaluating it\n",
    "\n",
    "We can now evaluate our model on the validation dataset. To do so, we first make the estimations $\\vec{\\hat{y}}_{validation}$ and then we compute the mse between it and the true values $\\vec{y}_{validation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a55b49",
   "metadata": {
    "id": "44a55b49"
   },
   "outputs": [],
   "source": [
    "y_hat_linreg_val = lin_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a7075",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a76a7075",
    "outputId": "1825f51f-a653-4cd6-9607-8ba83d8aeee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5957864533826812"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can load a function to compute the mse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_reg_mse_val = mean_squared_error(y_hat_linreg_val, y_val)\n",
    "lin_reg_mse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500da57",
   "metadata": {
    "id": "6500da57"
   },
   "source": [
    "If we were less lazy, or simply forgot about the existence of the `mean_squared_error` function, we could also compute easily the mse using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169282a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8169282a",
    "outputId": "d890e213-a6ce-4124-dc26-89f5fa324454"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5957864533826812"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_hat_linreg_val - y_val) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2040ac",
   "metadata": {
    "id": "5f2040ac"
   },
   "source": [
    "This measure of the error of our model is not very interpretable, the root mean square error (rmse) is better on that point. Let's compare it with the average value of $\\vec{y}_{validation}$ to evaluate a bit better the estimated error of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f287b7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f287b7f",
    "outputId": "e88a358c-19d5-487e-c9fe-957d01f60136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_val = 0.7718720446956744\n",
      "y_val_mean = 2.022349723837209\n",
      "rmse_val / y_val_mean = 0.38167090271168497\n"
     ]
    }
   ],
   "source": [
    "lin_reg_rmse_val = np.sqrt(lin_reg_mse_val)\n",
    "y_val_mean = np.mean(y_val)\n",
    "\n",
    "print(\"rmse_val =\", lin_reg_rmse_val)\n",
    "print(\"y_val_mean =\", y_val_mean)\n",
    "print(\"rmse_val / y_val_mean =\", lin_reg_rmse_val/y_val_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad9e1c",
   "metadata": {
    "id": "06ad9e1c"
   },
   "source": [
    "### Comparing our result with a heuristic model\n",
    "\n",
    "So, in average, the housing price for the validation dataset is about \\$200 000 and the estimated error is \\$35 000, which sounds reasonable. But to interpret more these results, and before testing a more complex ML algorithm, a good practice is to compare our result with a heuristic estimation. For example, we could choose $\\hat{y} = mean(\\vec{y}_{training})$, it is simple model that outputs a constant, the average value of the output values in the training dataset. This may sound ridiculously too simple, but sometimes you can spend hours trying to find the best ML algorithm and actually end up with a result that is not much better than a simple heuristic. This usually happens when the quality of the dataset is not good, i.e. when there is not much data and/or there is not much relation between the input and the output. Depending on the distribution of $y$, the median may be a better heuristic than the mean, so we will also verify the performance of the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3239a40",
   "metadata": {
    "id": "d3239a40"
   },
   "source": [
    "### How to compare the performance of different models ?\n",
    "\n",
    "To compare the performance of different models, you usually should not restrict yourself to only compare their respective loss values (the mse in our current regression problem). You can compute a multitude of metrics that can help you identify some problem in the performance of your model, that you would not be able to detect if you were only looking at the loss value otherwise. Scikit-learn allows you to compute a few popular metrics (https://scikit-learn.org/stable/modules/model_evaluation.html) but you do not have to restrict yourself to those. For example, in our housing prices problem, we could be particularly interested by models performing well for expensive houses, pricing above \\$300 000. Let's build a function allowing us to compute the rmse of those expensive houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b91bb7",
   "metadata": {
    "id": "56b91bb7"
   },
   "outputs": [],
   "source": [
    "def expensive_rmse(y_hat, y_true, thr_price=3.5):\n",
    "    \"\"\"\n",
    "    Return the rmse between the predicted house prices and the true house prices\n",
    "    for the houses pricing above the given price threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We compute a mask to identify houses more expensive than the threshold price\n",
    "    expensive_mask = y_true >= thr_price\n",
    "    \n",
    "    # We use the mask to keep only the houses pricing above the threshold\n",
    "    y_true = y_true[expensive_mask]\n",
    "    y_hat = y_hat[expensive_mask]\n",
    "    \n",
    "    # We compute the mse and return the root of the mse\n",
    "    mse = np.mean((y_hat - y_true) ** 2)\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cd8d7",
   "metadata": {
    "id": "bb1cd8d7"
   },
   "source": [
    "We now compute the loss value (the mse), the rmse, and the rmse of expensive houses on the validation dataset for the linear regression model, and the average and median heuristic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2ba92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17a2ba92",
    "outputId": "bddd7edc-3067-4799-d58b-11c04bfffeaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE\n",
      "LinReg: 0.5957864533826815\n",
      "Heuristic mean: 1.3065588360035056\n",
      "Heuristic median: 1.3516798199458815\n",
      "\n",
      "RMSE\n",
      "LinReg: 0.7718720446956746\n",
      "Heuristic mean: 1.1430480462358115\n",
      "Heuristic median: 1.162617658538645\n",
      "\n",
      "RMSE expensive houses\n",
      "LinReg: 1.5071673512167\n",
      "Heuristic mean: 2.3753399557929598\n",
      "Heuristic median: 2.63768469545378\n"
     ]
    }
   ],
   "source": [
    "# we already computed the mse and rmse for the lin reg model above\n",
    "# so we only have to compute the rmse for expensive houses using the lin reg model\n",
    "linreg_exp_rmse_val = expensive_rmse(y_hat_linreg_val, y_val)\n",
    "\n",
    "\n",
    "# Heuristic mean model.\n",
    "# we compute the training mean value of houses, then create a vector with the same dimension than\n",
    "# the y_val vector with the mean training value everywhere.\n",
    "y_train_mean = np.mean(y_train)\n",
    "y_hat_mean_val = np.ones_like(y_val) * y_train_mean\n",
    "\n",
    "# we compute the different metrics for the heuristic mean model\n",
    "heuristic_mean_mse_val = np.mean((y_hat_mean_val - y_val) ** 2)\n",
    "heuristic_mean_rmse_val = np.sqrt(heuristic_mean_mse_val)\n",
    "heuristic_mean_exp_rmse_val = expensive_rmse(y_hat_mean_val, y_val)\n",
    "\n",
    "# Same as above but for the heuristic median model\n",
    "y_train_median = np.median(y_train)\n",
    "y_hat_median_val = np.ones_like(y_val) * y_train_median\n",
    "\n",
    "heuristic_median_mse_val = np.mean((y_hat_median_val - y_val) ** 2)\n",
    "heuristic_median_rmse_val = np.sqrt(heuristic_median_mse_val)\n",
    "heuristic_median_exp_rmse_val = expensive_rmse(y_hat_median_val, y_val)\n",
    "\n",
    "# We print all results\n",
    "print(\"MSE\")\n",
    "print(\"LinReg:\", lin_reg_mse_val)\n",
    "print(\"Heuristic mean:\", heuristic_mean_mse_val)\n",
    "print(\"Heuristic median:\", heuristic_median_mse_val)\n",
    "\n",
    "print(\"\\nRMSE\")\n",
    "print(\"LinReg:\", lin_reg_rmse_val)\n",
    "print(\"Heuristic mean:\", heuristic_mean_rmse_val)\n",
    "print(\"Heuristic median:\", heuristic_median_rmse_val)\n",
    "\n",
    "print(\"\\nRMSE expensive houses\")\n",
    "print(\"LinReg:\", linreg_exp_rmse_val)\n",
    "print(\"Heuristic mean:\", heuristic_mean_exp_rmse_val)\n",
    "print(\"Heuristic median:\", heuristic_median_exp_rmse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e581e",
   "metadata": {
    "id": "fd4e581e"
   },
   "source": [
    "Thoses results are clearly showing that the linear model is performing much better than the heuristics, and the linear model being still a very simple model, we probably can try a more complex model. But before doing that, you can compare the training and validation loss value, which can allow us to verify if we are underfitting (the model is too simple for our dataset/problem) or overfitting (the model is too complex for our dataset/problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d858be4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d858be4",
    "outputId": "bfb94927-4fa1-47da-efe9-a87197b4edf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.5184038842448243\n",
      "validation loss: 0.5957864533826815\n"
     ]
    }
   ],
   "source": [
    "y_hat_linreg_train = lin_reg.predict(X_train)\n",
    "lin_reg_mse_train = np.mean((y_hat_linreg_train - y_train) ** 2)\n",
    "\n",
    "print(\"training loss:\", lin_reg_mse_train)\n",
    "print(\"validation loss:\", lin_reg_mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95b5f8",
   "metadata": {
    "id": "2f95b5f8"
   },
   "source": [
    "This is a big difference between the two loss values... But is it significant ? You may think so, but try to re-run the notebook after changing the random seed from 42 to 0, and check the new loss values found. You will then find a difference between the two loss not looking so significant! To be able to increase the statistical accuracy of our validation loss, we would need to increase the size of the validation dataset, but we do not want to reduce the training dataset size either... The solution in those cases is using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b2bc5",
   "metadata": {
    "id": "150b2bc5"
   },
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e8cdd",
   "metadata": {
    "id": "4b8e8cdd"
   },
   "source": [
    "![](figures/cross_validation.png)\n",
    "\n",
    "Image from: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5910ab7",
   "metadata": {
    "id": "f5910ab7"
   },
   "source": [
    "The principle of cross validation is to fuse the training and validation datasets together, then to divide this dataset in $n$ folds, typically $2 \\leq n \\leq 10$ (in the picture above n=5). Then, we train a model using all but one of the fold, which is then used to compute a validation loss. Repeating this for all folds, and then averaging all the different validation loss, we are able to compute the validation loss on much more data samples, and have a final validation loss that is more significant.\n",
    "\n",
    "Scikit learn offers different way to do cross validation, here we will just use a function to help us split our training and validation data into 5 folds, then do the cross validation ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efa0ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9efa0ed",
    "outputId": "fdaa01e6-5fb0-4ad9-8e05-20e5d242a9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training loss: 0.524 std: 0.005\n",
      "Mean validation loss: 0.534 std: 0.025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# we create a 5 folds splitting object that will return the training and validation indexes for our 5 folds\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# We fuse the training and validation set together\n",
    "X_trainval = np.concatenate((X_train, X_val))\n",
    "y_trainval = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Lists to save the losses\n",
    "train_mses = []\n",
    "val_mses = []\n",
    "\n",
    "# loop through the different folds\n",
    "for train_indexes, val_indexes in kf.split(X_trainval):\n",
    "    \n",
    "    # create an new linreg model, untrained\n",
    "    lin_reg_temp = LinearRegression()\n",
    "    # train the model on a subset of the training data\n",
    "    lin_reg_temp.fit(X_trainval[train_indexes], y_trainval[train_indexes])\n",
    "    \n",
    "    # make prediction for training and validation dataset\n",
    "    y_hat_train_temp = lin_reg_temp.predict(X_trainval[train_indexes])\n",
    "    y_hat_val_temp = lin_reg_temp.predict(X_trainval[val_indexes])\n",
    "    \n",
    "    # compute the losses\n",
    "    mse_train_temp = np.mean((y_hat_train_temp - y_trainval[train_indexes]) ** 2)\n",
    "    mse_val_temp = np.mean((y_hat_val_temp - y_trainval[val_indexes]) ** 2)\n",
    "    \n",
    "    # append the loss values to the lists\n",
    "    train_mses.append(mse_train_temp)\n",
    "    val_mses.append(mse_val_temp)\n",
    "    \n",
    "print(f\"Mean training loss: {round(np.mean(train_mses), 3)} std: {round(np.std(train_mses), 3)}\")\n",
    "print(f\"Mean validation loss: {round(np.mean(val_mses), 3)} std: {round(np.std(val_mses), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e08c1",
   "metadata": {
    "id": "a74e08c1"
   },
   "source": [
    "The two loss values are very close, with the validation loss bigger than the training loss, which is expected. In the case of underfitting, those two values are almost identical, so it is highly likely that our linear model is underfitting. \n",
    "\n",
    "Knowing that we are underfitting, we can try more complex non-linear models. This would be a good exercise to try at least 2 other more complex models, compare their performance on the validation dataset and try to tell if they are overfitting or not. And finally, evaluate the best model on the test dataset to have a final evaluation of its performance. Before evaluating the best model on your test set, you should re-train it on the merged training-validation dataset, to maximize your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935b93c",
   "metadata": {
    "id": "b935b93c"
   },
   "source": [
    "## Classification\n",
    "\n",
    "After having seen an example of a regression problem, we will now look at an example of classification problem. For a classification, the output data does not take a continuous value, it is instead a label, or a class. As an example for a classification problem, we will use the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa62e87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfa62e87",
    "outputId": "b285f6f2-0995-46e6-c625-a303766d9e09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_ds = load_iris()\n",
    "print(iris_ds.keys())\n",
    "print(iris_ds[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796defd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e796defd",
    "outputId": "face9dc7-a34a-4e00-8281-950d5f5dda1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "y shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "X = iris_ds[\"data\"]\n",
    "y = iris_ds[\"target\"]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529101b",
   "metadata": {
    "id": "3529101b"
   },
   "source": [
    "As you can see from the description, the output data can take 3 different values, which are 3 types of flower. Those three classes are Iris-Setosa, Iris-Versicolour and Iris-Virginica. The input data has 4 attributes, which are the length and width measurements of the sepal and petal of each flower, and there are measurements for 150 flowers in the dataset. \n",
    "\n",
    "### Statistical study of a classification dataset\n",
    "\n",
    "The statistics about the input data and output data are already given in the description. In a classification problem, the frequency of appearance for each class in the dataset is very important. In this dataset, the three classes are balanced, there 50 appearances for each of the 3 classes. Usually this is not the case and it can be a problem. For example, a model trained on a dataset where one class is under-represented will usually have more trouble to correctly classify this particular class. Also, if the dataset is not balanced, you will need to look at different metrics to correctly assess the performance of a model. For example, the accuracy of a model is a performance metric that is not necessary very informative when a dataset is very imbalanced and the F1 score is much more useful in that case (https://en.wikipedia.org/wiki/F-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6bcba",
   "metadata": {
    "id": "4cf6bcba"
   },
   "source": [
    "When dealing with categorical data, the data is usually represented as an integer value $k=0,...,K-1$ where K the number of different category/class existing. Sometimes the data has a string format, and the value have the name of the category. When this is the case, we first need to convert the classes from string to an integer value. As you can see below, the values of $y$ are already integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329bace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9329bace",
    "outputId": "ec57f2f9-5554-4c82-b04b-2c01461e6db4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6743d9",
   "metadata": {
    "id": "af6743d9"
   },
   "source": [
    "### Splitting the dataset into training and test set\n",
    "\n",
    "There are only 150 data samples, so we will use cross-validation instead of splitting our dataset into a training, a validation and test set. What ratio between training and testing should we choose ? Anything under 30 data samples for testing is going to be too low to evaluate properly our dataset. I will choose to keep 50 data samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ae003",
   "metadata": {
    "id": "966ae003"
   },
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121c0d7",
   "metadata": {
    "id": "5121c0d7"
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "We will use the classifier version of the linear regression model used previously: the logistic regression. This linear model predict a probability a score for each class, and then select the class with the highest estimated probability as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7c570",
   "metadata": {
    "id": "e9f7c570"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bb991",
   "metadata": {
    "id": "960bb991"
   },
   "outputs": [],
   "source": [
    "# scikit-learn offer functions to compute different metrics, such as the accuracy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865441c6",
   "metadata": {
    "id": "865441c6"
   },
   "source": [
    "When the dataset is very small, it is better use cross validation with a higher number of folds, so that we can use a higher amount of training sample. The reason we do not always use a high number of fold is to reduce the computing time. Indeed, for each fold you have, you need to retrain a model, which can be quite long depending on the dataset size and the ML algorithm used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd1e2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1dd1e2d",
    "outputId": "b94cc158-004e-4739-c60c-3d86079046c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training accuracy: 0.978 std: 0.007\n",
      "Mean validation accuracy: 0.96 std: 0.049\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(10)\n",
    "\n",
    "train_folds_acc = []\n",
    "val_folds_acc = []\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_trainval):\n",
    "    \n",
    "    # Initialize a new logreg classifier and train it\n",
    "    log_reg_clf = LogisticRegression(max_iter=200)\n",
    "    log_reg_clf.fit(X_trainval[train_indexes], y_trainval[train_indexes])\n",
    "    \n",
    "    # make prediction for training and validation dataset\n",
    "    y_hat_train_temp = log_reg_clf.predict(X_trainval[train_indexes])\n",
    "    y_hat_val_temp = log_reg_clf.predict(X_trainval[val_indexes])\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_train_temp = accuracy_score(y_hat_train_temp, y_trainval[train_indexes])\n",
    "    acc_val_temp = accuracy_score(y_hat_val_temp, y_trainval[val_indexes])\n",
    "    \n",
    "    # append the accuracy values to the lists\n",
    "    train_folds_acc.append(acc_train_temp)\n",
    "    val_folds_acc.append(acc_val_temp)\n",
    "    \n",
    "print(f\"Mean training accuracy: {round(np.mean(train_folds_acc), 3)} std: {round(np.std(train_folds_acc), 3)}\")\n",
    "print(f\"Mean validation accuracy: {round(np.mean(val_folds_acc), 3)} std: {round(np.std(val_folds_acc), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b993feb",
   "metadata": {
    "id": "7b993feb"
   },
   "source": [
    "The accuracy obtained is pretty good, both on the training and validation set. This type of small dataset is very easy to overfit if you use a more complex model, and you could then obtain 100% accuracy on the training dataset. It is not the case here and the validation accuracy is very close too, so we are not overfitting and we may be able to improve a bit more the accuracy by using a more complex model, which you can try to do.\n",
    "\n",
    "### Final evaluation on test set\n",
    "\n",
    "Let's assume we do not want to experiment with another model, that we are set on using the logistic regression model, we can now re-train the model on the merged training and validation set, and then evaluate it on the test set. \n",
    "\n",
    "Like already said, it is better to not restrict ourself to only one metric for evaluating a model. Scikit-learn has a good range of metrics available (https://scikit-learn.org/stable/modules/model_evaluation.html). A useful one for example is the classification_report, reporting the precision, recall and f1 score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3f91f",
   "metadata": {
    "id": "f8f3f91f"
   },
   "outputs": [],
   "source": [
    "# training a model on the training-valdiation dataset\n",
    "log_reg_clf = LogisticRegression(max_iter=200)\n",
    "log_reg_clf.fit(X_trainval, y_trainval)\n",
    "\n",
    "# make prediction for the test dataset\n",
    "y_hat_test = log_reg_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be17c2",
   "metadata": {
    "id": "53be17c2",
    "outputId": "0ddcdef9-b0db-4c6b-ca36-0a2a962ef266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.94      0.94      0.94        17\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.96        50\n",
      "   macro avg       0.96      0.96      0.96        50\n",
      "weighted avg       0.96      0.96      0.96        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_hat_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
